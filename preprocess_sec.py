"""
SEC æ–‡ä»¶é¢„å¤„ç†è„šæœ¬

åŠŸèƒ½ï¼š
1. ä¸‹è½½å…¬å¸çš„ SEC filings (10-K, 10-Q, DEF 14A, S-1, 8-K)
2. æå–å…³é”®ç« èŠ‚ä¸º markdown æ–‡ä»¶
3. æ™ºèƒ½å‹ç¼©å†…å®¹ï¼Œå‡å°‘ token æ¶ˆè€—
4. ç”Ÿæˆ _index.json ç´¢å¼•æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python preprocess_sec.py TICKER [--filings 10-K,10-Q,DEF-14A]

è¾“å‡ºç»“æ„ï¼š
    files/{TICKER}/
    â”œâ”€â”€ _index.json              # ç´¢å¼•æ–‡ä»¶
    â”œâ”€â”€ filings/                  # åŸå§‹ HTML æ–‡ä»¶
    â”‚   â””â”€â”€ {date}_{type}_{accession}/
    â”‚       â”œâ”€â”€ *.htm
    â”‚       â””â”€â”€ metadata.json
    â””â”€â”€ raw/                      # é¢„å¤„ç†åçš„ markdown
        â””â”€â”€ {date}_{type}/
            â”œâ”€â”€ item1_business.md
            â”œâ”€â”€ item1a_risk_factors.md
            â”œâ”€â”€ item7_mda.md
            â””â”€â”€ financial_snapshot.json
"""

import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

load_dotenv()

from tools.sec_tools import SECTools


# ============ å†…å®¹å‹ç¼©é…ç½® ============

# æ¯ä¸ªç« èŠ‚çš„æœ€å¤§å­—ç¬¦æ•°é™åˆ¶
MAX_SECTION_CHARS = {
    "Item 1": 20000,  # Business - ä¿ç•™æ›´å¤š
    "Item 1A": 15000,  # Risk Factors - å‹ç¼©æœ€å¤š
    "Item 2": 15000,  # MD&A (10-Q)
    "Item 7": 20000,  # MD&A (10-K) - ä¿ç•™æ›´å¤š
    "Item 7A": 10000,  # Market Risk
    "Item 8": 15000,  # Financials
}

# è¦åˆ é™¤çš„æ³•å¾‹æ¨¡æ¿è¯­è¨€æ¨¡å¼
BOILERPLATE_PATTERNS = [
    # å¸¸è§çš„æ³•å¾‹å…è´£å£°æ˜
    r"may have a material adverse effect on our business,? financial condition,? and results of operations",
    r"could have a material adverse effect on our business,? financial condition,? and results of operations",
    r"which could harm our business,? financial condition and results of operations",
    r"could negatively impact our business and financial results",
    r"may negatively impact our business and financial results",
    r"could materially and adversely affect our business",
    r"may materially and adversely affect our business",
    r"there can be no assurance that",
    r"we cannot guarantee that",
    r"we may not be able to",
    # é‡å¤çš„æ—¶é—´è¡¨è¾¾
    r"in the future",
    r"from time to time",
]

# ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
BOILERPLATE_REGEX = [re.compile(p, re.IGNORECASE) for p in BOILERPLATE_PATTERNS]


def compress_section_content(content: str, section_name: str) -> str:
    """
    æ™ºèƒ½å‹ç¼©ç« èŠ‚å†…å®¹ï¼Œå‡å°‘ token æ¶ˆè€—ä½†ä¿ç•™å…³é”®ä¿¡æ¯ã€‚

    ç­–ç•¥ï¼š
    1. åˆ é™¤é‡å¤çš„æ³•å¾‹æ¨¡æ¿è¯­è¨€
    2. å‹ç¼©å¤šä½™ç©ºç™½
    3. å¦‚æœä»ç„¶è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰æ®µè½æˆªå–
    """
    if not content:
        return content

    original_len = len(content)
    max_chars = MAX_SECTION_CHARS.get(section_name, 15000)

    # å¦‚æœå†…å®¹å·²ç»å¾ˆçŸ­ï¼Œä¸éœ€è¦å‹ç¼©
    if len(content) <= max_chars:
        return content

    # Step 1: åˆ é™¤å¸¸è§çš„æ³•å¾‹æ¨¡æ¿è¯­è¨€ï¼ˆæ›¿æ¢ä¸ºç®€çŸ­ç‰ˆæœ¬ï¼‰
    for pattern in BOILERPLATE_REGEX:
        content = pattern.sub("", content)

    # Step 2: å‹ç¼©å¤šä½™ç©ºç™½
    content = re.sub(r"\n{3,}", "\n\n", content)  # å¤šä¸ªæ¢è¡Œå‹ç¼©ä¸ºä¸¤ä¸ª
    content = re.sub(r" {2,}", " ", content)  # å¤šä¸ªç©ºæ ¼å‹ç¼©ä¸ºä¸€ä¸ª
    content = re.sub(r"\t+", " ", content)  # Tab æ›¿æ¢ä¸ºç©ºæ ¼

    # Step 3: åˆ é™¤é‡å¤çš„ bullet points å‰ç¼€
    content = re.sub(r"(â€¢ ){2,}", "â€¢ ", content)

    # Step 4: å¦‚æœä»ç„¶è¶…è¿‡é™åˆ¶ï¼Œæ™ºèƒ½æˆªå–
    if len(content) > max_chars:
        # å°è¯•åœ¨æ®µè½è¾¹ç•Œæˆªå–
        paragraphs = content.split("\n\n")
        truncated = []
        current_len = 0

        for para in paragraphs:
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šå·²æœ‰å†…å®¹è¶…è¿‡é™åˆ¶
            if current_len + len(para) + 2 > max_chars:
                # å¦‚æœè¿˜æ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œè‡³å°‘ä¿ç•™éƒ¨åˆ†ç¬¬ä¸€æ®µ
                if current_len == 0:
                    remaining = max_chars - 100  # ç•™ç©ºé—´ç»™æˆªæ–­è¯´æ˜
                    truncated.append(para[:remaining] + "...")
                    current_len = remaining
                # æ·»åŠ æˆªæ–­è¯´æ˜
                truncated.append(
                    f"\n\n[... å†…å®¹å·²å‹ç¼©ï¼ŒåŸæ–‡ {original_len:,} å­—ç¬¦ â†’ {current_len:,} å­—ç¬¦ ...]"
                )
                break
            truncated.append(para)
            current_len += len(para) + 2

        content = "\n\n".join(truncated)

    return content


PROJECT_ROOT = Path(__file__).resolve().parent
FILES_ROOT = PROJECT_ROOT / "files"

# æ¯ç§ filing ç±»å‹è¦æå–çš„ç« èŠ‚
FILING_SECTIONS = {
    "10-K": ["Item 1", "Item 1A", "Item 7", "Item 7A", "Item 8"],
    "10-Q": ["Item 1", "Item 2", "Item 1A"],
    "DEF 14A": [],  # Proxy éœ€è¦ç‰¹æ®Šå¤„ç†
    "S-1": ["Item 1", "Item 1A", "Item 7"],
    "8-K": [],  # 8-K ç»“æ„ä¸å›ºå®š
}

# ç« èŠ‚åç§°æ˜ å°„ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
SECTION_FILENAME_MAP = {
    "Item 1": "item1_business",
    "Item 1A": "item1a_risk_factors",
    "Item 2": "item2_mda",
    "Item 7": "item7_mda",
    "Item 7A": "item7a_market_risk",
    "Item 8": "item8_financials",
}


def preprocess_ticker(
    ticker: str,
    filing_types: list[str] | None = None,
    verbose: bool = True,
) -> dict[str, Any]:
    """
    é¢„å¤„ç†æŒ‡å®š ticker çš„ SEC æ–‡ä»¶ã€‚

    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        filing_types: è¦å¤„ç†çš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ ["10-K", "10-Q", "DEF 14A"]
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦ä¿¡æ¯

    Returns:
        ç´¢å¼•å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¤„ç†è¿‡çš„æ–‡ä»¶ä¿¡æ¯
    """
    if filing_types is None:
        filing_types = ["10-K", "10-Q", "DEF 14A"]

    ticker = ticker.upper()
    tools = SECTools()

    ticker_dir = FILES_ROOT / ticker
    raw_dir = ticker_dir / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)

    index = {
        "ticker": ticker,
        "preprocessed_at": datetime.now().isoformat(),
        "filings": [],
    }

    if verbose:
        print(f"\n{'='*60}")
        print(f"é¢„å¤„ç† {ticker} çš„ SEC æ–‡ä»¶")
        print(f"{'='*60}")

    # è·å–å„ç±»å‹æœ€æ–° filing
    for filing_type in filing_types:
        if verbose:
            print(f"\nğŸ“„ è·å– {filing_type}...")

        try:
            if filing_type in ["10-K", "10-K/A"]:
                filing = tools.get_latest_10k(ticker)
            elif filing_type in ["10-Q", "10-Q/A"]:
                filing = tools.get_latest_10q(ticker)
            elif filing_type == "DEF 14A":
                filing = tools.get_latest_proxy(ticker)
            elif filing_type in ["S-1", "S-1/A", "424B4", "424B3", "424B2", "424B1"]:
                filing = tools.get_latest_s1(ticker)
            else:
                if verbose:
                    print(f"  âš ï¸ ä¸æ”¯æŒçš„ç±»å‹: {filing_type}")
                continue

            if not filing:
                if verbose:
                    print(f"  âš ï¸ æœªæ‰¾åˆ° {filing_type}")
                continue

            if verbose:
                print(
                    f"  âœ“ æ‰¾åˆ°: {filing.get('filing_date')} ({filing.get('accession_number')})"
                )

            # åˆ›å»ºè¾“å‡ºç›®å½•
            report_date = (
                filing.get("report_date") or filing.get("filing_date") or "unknown"
            )
            output_dir = raw_dir / f"{report_date}_{filing_type.replace('/', '-')}"
            output_dir.mkdir(parents=True, exist_ok=True)

            filing_info = {
                "type": filing_type,
                "filing_date": filing.get("filing_date"),
                "report_date": filing.get("report_date"),
                "accession_number": filing.get("accession_number"),
                "local_path": filing.get("local_path"),
                "extracted_sections": [],
            }

            # æå–ç« èŠ‚
            local_path = filing.get("local_path")
            if local_path and Path(local_path).exists():
                sections_to_extract = FILING_SECTIONS.get(filing_type, [])

                if sections_to_extract:
                    if verbose:
                        print(f"  ğŸ“‘ æå–ç« èŠ‚: {', '.join(sections_to_extract)}")

                    result = tools.extract_sec_sections(local_path, sections_to_extract)
                    sections = result.get("sections", {})

                    for section_name, content in sections.items():
                        if "[Section" in content and "not found" in content:
                            continue

                        # ç”Ÿæˆæ–‡ä»¶å
                        filename = SECTION_FILENAME_MAP.get(
                            section_name, section_name.lower().replace(" ", "_")
                        )
                        md_path = output_dir / f"{filename}.md"

                        # å‹ç¼©å†…å®¹
                        original_len = len(content)
                        compressed_content = compress_section_content(
                            content, section_name
                        )
                        compressed_len = len(compressed_content)

                        # å†™å…¥ markdown
                        md_content = f"# {section_name}\n\n"
                        md_content += f"**Source**: {filing_type} ({report_date})\n"
                        md_content += (
                            f"**Accession**: {filing.get('accession_number')}\n"
                        )
                        if compressed_len < original_len:
                            md_content += f"**Compressed**: {original_len:,} â†’ {compressed_len:,} chars ({100 - compressed_len * 100 // original_len}% reduction)\n"
                        md_content += "\n---\n\n"
                        md_content += compressed_content

                        md_path.write_text(md_content, encoding="utf-8")
                        filing_info["extracted_sections"].append(
                            {
                                "section": section_name,
                                "file": str(md_path.relative_to(ticker_dir)),
                                "size_bytes": compressed_len,
                                "original_size_bytes": original_len,
                            }
                        )

                        if verbose:
                            if compressed_len < original_len:
                                reduction = 100 - compressed_len * 100 // original_len
                                print(
                                    f"    âœ“ {section_name} â†’ {md_path.name} ({original_len:,} â†’ {compressed_len:,} bytes, -{reduction}%)"
                                )
                            else:
                                print(
                                    f"    âœ“ {section_name} â†’ {md_path.name} ({compressed_len:,} bytes)"
                                )

            # è·å–è´¢åŠ¡å¿«ç…§
            if filing_type in ["10-K", "10-Q"]:
                if verbose:
                    print(f"  ğŸ“Š è·å–è´¢åŠ¡æ•°æ®...")
                try:
                    snapshot = tools.extract_financial_tables(filing["url"])
                    snapshot_path = output_dir / "financial_snapshot.json"
                    snapshot_path.write_text(
                        json.dumps(snapshot, indent=2), encoding="utf-8"
                    )
                    filing_info["financial_snapshot"] = str(
                        snapshot_path.relative_to(ticker_dir)
                    )
                    if verbose:
                        print(f"    âœ“ è´¢åŠ¡å¿«ç…§ â†’ financial_snapshot.json")
                except Exception as e:
                    if verbose:
                        print(f"    âš ï¸ è´¢åŠ¡æ•°æ®è·å–å¤±è´¥: {e}")

            index["filings"].append(filing_info)

        except Exception as e:
            if verbose:
                print(f"  âŒ é”™è¯¯: {e}")
            continue

    # ä¿å­˜ç´¢å¼•
    index_path = ticker_dir / "_index.json"
    index_path.write_text(
        json.dumps(index, indent=2, ensure_ascii=False), encoding="utf-8"
    )

    if verbose:
        print(f"\n{'='*60}")
        print(f"âœ… é¢„å¤„ç†å®Œæˆ")
        print(f"   ç´¢å¼•æ–‡ä»¶: {index_path}")
        print(f"   å¤„ç†æ–‡ä»¶æ•°: {len(index['filings'])}")
        total_sections = sum(
            len(f.get("extracted_sections", [])) for f in index["filings"]
        )
        print(f"   æå–ç« èŠ‚æ•°: {total_sections}")
        print(f"{'='*60}\n")

    return index


def main():
    parser = argparse.ArgumentParser(
        description="é¢„å¤„ç† SEC æ–‡ä»¶ï¼Œæå–å…³é”®ç« èŠ‚ä¸º markdown"
    )
    parser.add_argument(
        "ticker",
        help="è‚¡ç¥¨ä»£ç ï¼Œå¦‚ NVDA, VEEV",
    )
    parser.add_argument(
        "--filings",
        default="10-K,10-Q,DEF 14A",
        help="è¦å¤„ç†çš„æ–‡ä»¶ç±»å‹ï¼Œé€—å·åˆ†éš” (é»˜è®¤: 10-K,10-Q,DEF 14A)",
    )
    parser.add_argument(
        "--quiet",
        "-q",
        action="store_true",
        help="å®‰é™æ¨¡å¼ï¼Œä¸æ‰“å°è¿›åº¦",
    )

    args = parser.parse_args()

    filing_types = [f.strip() for f in args.filings.split(",")]

    try:
        index = preprocess_ticker(
            ticker=args.ticker,
            filing_types=filing_types,
            verbose=not args.quiet,
        )

        if args.quiet:
            print(json.dumps(index, indent=2, ensure_ascii=False))

    except Exception as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
