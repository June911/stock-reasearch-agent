#!/usr/bin/env python3
"""
SEC Filing Parser - å°† SEC HTML æ–‡ä»¶é¢„å¤„ç†ä¸ºç»“æ„åŒ– Markdown

åŠŸèƒ½ï¼š
1. ä» filings/ ç›®å½•è¯»å–åŸå§‹ HTML
2. æŒ‰ Item åˆ‡åˆ†ç« èŠ‚ï¼Œè¾“å‡ºåˆ° raw/ ç›®å½•
3. ç”Ÿæˆ _index.json å…ƒæ•°æ®ç´¢å¼•

ä½¿ç”¨æ–¹æ³•ï¼š
    python -m tools.sec_parser --ticker CRCL
    python -m tools.sec_parser --ticker CRCL --filing 2025-09-30_10-Q
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv

load_dotenv()

from .sec_tools import SECTools

PROJECT_ROOT = Path(__file__).resolve().parent.parent
FILES_ROOT = PROJECT_ROOT / "files"

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


class SECParser:
    """SEC è´¢æŠ¥è§£æå™¨ï¼Œå°† HTML è½¬ä¸ºç»“æ„åŒ– Markdown æ–‡ä»¶ã€‚"""

    # Item æ˜ å°„ï¼šItem åç§° -> è¾“å‡ºæ–‡ä»¶å
    ITEM_FILE_MAP = {
        "Item 1": "item1.md",  # 10-Q: Financial Statements / 10-K: Business
        "Item 1A": "item1a_risks.md",
        "Item 1B": "item1b_comments.md",
        "Item 2": "item2.md",  # 10-Q: MD&A / 10-K: Properties
        "Item 3": "item3.md",
        "Item 4": "item4.md",
        "Item 5": "item5_other_info.md",  # Part II: Other Info (é«˜ç®¡äº¤æ˜“è®¡åˆ’ç­‰)
        "Item 6": "item6.md",
        "Item 7": "item7_mda.md",
        "Item 7A": "item7a_market_risk.md",
        "Item 8": "item8_financials.md",
    }

    # S-1/424B4 æ‹›è‚¡ä¹¦ç« èŠ‚æ˜ å°„
    S1_SECTION_MAP = {
        "Prospectus Summary": "prospectus_summary.md",
        "Risk Factors": "risk_factors.md",
        "Use of Proceeds": "use_of_proceeds.md",
        "Business": "business.md",
        "Management's Discussion": "mda.md",
        "Management": "management.md",
        "Executive Compensation": "executive_compensation.md",
        "Certain Relationships": "related_party.md",
        "Principal Stockholders": "principal_stockholders.md",
        "Description of Capital Stock": "capital_stock.md",
    }

    # 10-K å’Œ 10-Q çš„é»˜è®¤æå– Items
    DEFAULT_ITEMS_10K = ["Item 1", "Item 1A", "Item 7", "Item 7A"]
    DEFAULT_ITEMS_10Q = ["Item 1", "Item 1A", "Item 2", "Item 3", "Item 5"]  # Item 5: é«˜ç®¡äº¤æ˜“è®¡åˆ’

    # S-1/424B4 é»˜è®¤æå–ç« èŠ‚
    DEFAULT_SECTIONS_S1 = [
        "Prospectus Summary",
        "Risk Factors",
        "Business",
        "Management's Discussion",
        "Management",
        "Executive Compensation",
        "Principal Stockholders",
    ]

    def __init__(self, ticker: str):
        self.ticker = ticker.upper()
        self.ticker_dir = FILES_ROOT / self.ticker
        self.filings_dir = self.ticker_dir / "filings"
        self.raw_dir = self.ticker_dir / "raw"
        self.index_path = self.ticker_dir / "_index.json"
        self.sec_tools = SECTools()

    def parse_all(self) -> Dict[str, Any]:
        """è§£æ ticker ä¸‹æ‰€æœ‰ filingsã€‚"""
        if not self.filings_dir.exists():
            logger.error(f"Filings ç›®å½•ä¸å­˜åœ¨: {self.filings_dir}")
            return {"error": "filings directory not found"}

        results = []
        for filing_dir in sorted(self.filings_dir.iterdir()):
            if filing_dir.is_dir():
                result = self.parse_filing(filing_dir.name)
                if "error" not in result:
                    results.append(result)

        # æ›´æ–°ç´¢å¼•
        self._update_index(results)

        return {
            "ticker": self.ticker,
            "parsed_count": len(results),
            "filings": results,
        }

    def parse_filing(self, filing_folder: str) -> Dict[str, Any]:
        """
        è§£æå•ä¸ª filing ç›®å½•ã€‚

        Args:
            filing_folder: filing ç›®å½•åï¼Œå¦‚ "2025-09-30_10-Q_000187604225000047"

        Returns:
            è§£æç»“æœå­—å…¸
        """
        filing_path = self.filings_dir / filing_folder
        if not filing_path.exists():
            return {"error": f"Filing ç›®å½•ä¸å­˜åœ¨: {filing_path}"}

        # è§£æç›®å½•åè·å–å…ƒæ•°æ®
        meta = self._parse_folder_name(filing_folder)
        if not meta:
            return {"error": f"æ— æ³•è§£æç›®å½•å: {filing_folder}"}

        # æŸ¥æ‰¾ HTML æ–‡ä»¶
        html_file = self._find_html_file(filing_path)
        if not html_file:
            return {"error": f"æœªæ‰¾åˆ° HTML æ–‡ä»¶: {filing_path}"}

        # è¯»å– metadata.json è¡¥å……ä¿¡æ¯
        metadata_file = filing_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                file_meta = json.load(f)
                meta["filing_date"] = file_meta.get("filing_date")
                meta["accession_number"] = file_meta.get("accession_number")

        # ç¡®å®šè¦æå–çš„ Items/Sections
        filing_type = meta["filing_type"]
        is_s1 = filing_type in ["S-1", "S-1/A", "424B4", "424B1"]

        if is_s1:
            sections_to_extract = self.DEFAULT_SECTIONS_S1
            section_map = self.S1_SECTION_MAP
        elif filing_type in ["10-K", "10-K/A"]:
            sections_to_extract = self.DEFAULT_ITEMS_10K
            section_map = self.ITEM_FILE_MAP
        else:
            sections_to_extract = self.DEFAULT_ITEMS_10Q
            section_map = self.ITEM_FILE_MAP

        # æå– sections
        logger.info(f"ğŸ“„ è§£æ: {filing_folder}")

        if is_s1:
            # S-1 ä½¿ç”¨ä¸“é—¨çš„æå–æ–¹æ³•
            extract_result = self._extract_s1_sections(str(html_file), sections_to_extract)
        else:
            extract_result = self.sec_tools.extract_sec_sections(
                str(html_file), sections=sections_to_extract
            )

        if "error" in extract_result:
            return {"error": extract_result["error"]}

        # åˆ›å»ºè¾“å‡ºç›®å½•
        raw_subdir = f"{meta['report_date']}_{meta['filing_type'].replace('/', '-')}"
        output_dir = self.raw_dir / raw_subdir
        output_dir.mkdir(parents=True, exist_ok=True)

        # å†™å…¥ Markdown æ–‡ä»¶
        sections_written = []
        for section_name, content in extract_result.get("sections", {}).items():
            if content.startswith("[Section"):
                # Section æœªæ‰¾åˆ°ï¼Œè·³è¿‡
                continue

            safe_name = section_name.lower().replace(" ", "_").replace("'", "")
            filename = section_map.get(section_name, f"{safe_name}.md")
            output_file = output_dir / filename

            # æ·»åŠ å…ƒæ•°æ®å¤´
            header = f"# {section_name}\n\n"
            header += f"> Source: {html_file.name}\n"
            header += f"> Report Date: {meta['report_date']}\n"
            header += f"> Filing Type: {meta['filing_type']}\n\n"
            header += "---\n\n"

            file_size = len(header) + len(content)
            output_file.write_text(header + content, encoding="utf-8")
            sections_written.append({
                "name": filename.replace(".md", ""),
                "chars": file_size,
                "tokens_est": file_size // 4,  # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 chars
            })
            logger.info(f"  âœ… {section_name} -> {filename} ({len(content):,} chars)")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_raw_chars = sum(s["chars"] for s in sections_written)
        total_raw_tokens = sum(s["tokens_est"] for s in sections_written)
        source_file_size = html_file.stat().st_size
        source_tokens_est = source_file_size // 4

        result = {
            "report_date": meta["report_date"],
            "filing_type": meta["filing_type"],
            "filing_date": meta.get("filing_date"),
            "fiscal_quarter": self._infer_quarter(meta["report_date"]),
            "fiscal_year": meta["report_date"][:4],
            "source_file": f"filings/{filing_folder}/{html_file.name}",
            "raw_dir": f"raw/{raw_subdir}/",
            "sections": [s["name"] for s in sections_written],
            "processed_at": datetime.now().isoformat(),
            # Token ç»Ÿè®¡
            "stats": {
                "source_size_bytes": source_file_size,
                "source_tokens_est": source_tokens_est,
                "extracted_chars": total_raw_chars,
                "extracted_tokens_est": total_raw_tokens,
                "token_savings_pct": round((1 - total_raw_tokens / source_tokens_est) * 100, 1) if source_tokens_est > 0 else 0,
            },
            "sections_detail": sections_written,
        }

        return result

    def _parse_folder_name(self, folder_name: str) -> Optional[Dict[str, str]]:
        """
        è§£æ filing ç›®å½•åã€‚

        æ ¼å¼: {report_date}_{filing_type}_{accession}
        ç¤ºä¾‹: 2025-09-30_10-Q_000187604225000047
        """
        parts = folder_name.split("_")
        if len(parts) < 2:
            return None

        report_date = parts[0]
        filing_type = parts[1]

        # éªŒè¯æ—¥æœŸæ ¼å¼
        try:
            datetime.strptime(report_date, "%Y-%m-%d")
        except ValueError:
            return None

        return {
            "report_date": report_date,
            "filing_type": filing_type,
        }

    def _find_html_file(self, filing_path: Path) -> Optional[Path]:
        """æŸ¥æ‰¾ filing ç›®å½•ä¸‹çš„ä¸» HTML æ–‡ä»¶ã€‚"""
        # ä¼˜å…ˆæŸ¥æ‰¾ .htm æ–‡ä»¶
        htm_files = list(filing_path.glob("*.htm"))
        if htm_files:
            # æ’é™¤ metadataï¼Œé€‰æ‹©æœ€å¤§çš„æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ä¸»æ–‡æ¡£ï¼‰
            htm_files = [f for f in htm_files if "metadata" not in f.name.lower()]
            if htm_files:
                return max(htm_files, key=lambda f: f.stat().st_size)

        # å…¶æ¬¡æŸ¥æ‰¾ .html æ–‡ä»¶
        html_files = list(filing_path.glob("*.html"))
        if html_files:
            html_files = [f for f in html_files if "metadata" not in f.name.lower()]
            if html_files:
                return max(html_files, key=lambda f: f.stat().st_size)

        return None

    def _extract_s1_sections(
        self, file_path: str, sections: List[str]
    ) -> Dict[str, Any]:
        """
        ä» S-1/424B4 æ‹›è‚¡ä¹¦ä¸­æå–ç« èŠ‚ã€‚

        S-1 ä½¿ç”¨ç« èŠ‚æ ‡é¢˜è€Œé Item ç¼–å·ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ã€‚
        """
        from html.parser import HTMLParser

        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            return {"error": f"File not found: {file_path}", "sections": {}}

        try:
            html_content = file_path_obj.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            return {"error": f"Failed to read file: {str(e)}", "sections": {}}

        # æ¸…ç† HTML
        html_content = re.sub(
            r"<ix:header>.*?</ix:header>", "", html_content, flags=re.DOTALL | re.IGNORECASE
        )

        # æå–æ–‡æœ¬
        class TextExtractor(HTMLParser):
            def __init__(self):
                super().__init__()
                self.text = []
                self.skip = False

            def handle_starttag(self, tag, attrs):
                if tag.lower() in {"script", "style"} or tag.lower().startswith("ix:"):
                    self.skip = True

            def handle_endtag(self, tag):
                if tag.lower() in {"script", "style"} or tag.lower().startswith("ix:"):
                    self.skip = False

            def handle_data(self, data):
                if not self.skip:
                    cleaned = data.strip()
                    if cleaned:
                        self.text.append(cleaned)

        parser = TextExtractor()
        parser.feed(html_content)
        full_text = " ".join(parser.text)

        # S-1 ç« èŠ‚æŒ‰é¡ºåºæ’åˆ—ï¼Œç”¨äºç¡®å®šè¾¹ç•Œ
        # æ ¼å¼: (ç« èŠ‚å, èµ·å§‹æ¨¡å¼, ç»“æŸæ¨¡å¼)
        s1_section_order = [
            ("Prospectus Summary", r"Prospectus\s+summary\s+This\s+summary\s+highlights", r"RISK\s+FACTORS"),
            ("Risk Factors", r"RISK\s+FACTORS\s+(?:Investing|You\s+should|An\s+investment)", r"(?:CAUTIONARY|FORWARD.LOOKING|USE\s+OF\s+PROCEEDS)"),
            ("Use of Proceeds", r"USE\s+OF\s+PROCEEDS", r"DIVIDEND\s+POLICY"),
            ("Business", r"BUSINESS\s+(?:Overview|Our\s+Mission|Founded|Circle)", r"MANAGEMENT(?!\s*[''`]S)"),
            ("Management's Discussion", r"MANAGEMENT.S\s+DISCUSSION\s+AND\s+ANALYSIS", r"(?:BUSINESS\s+Overview|BUSINESS\s+Our|BUSINESS\s+Founded)"),
            ("Management", r"MANAGEMENT\s+(?:The\s+following|Executive\s+Officers|Our\s+executive)", r"EXECUTIVE\s+COMPENSATION"),
            ("Executive Compensation", r"EXECUTIVE\s+COMPENSATION", r"CERTAIN\s+RELATIONSHIPS"),
            ("Certain Relationships", r"CERTAIN\s+RELATIONSHIPS", r"(?:PRINCIPAL|SECURITY\s+OWNERSHIP|BENEFICIAL)"),
            ("Principal Stockholders", r"(?:PRINCIPAL\s+STOCKHOLDERS|SECURITY\s+OWNERSHIP|BENEFICIAL\s+OWNERSHIP)", r"DESCRIPTION\s+OF"),
            ("Description of Capital Stock", r"DESCRIPTION\s+OF\s+(?:CAPITAL|OUR\s+CAPITAL|SECURITIES)", r"(?:SHARES\s+ELIGIBLE|MATERIAL\s+U\.?S)"),
        ]

        extracted_sections = {}

        for section_name, start_pattern, end_pattern in s1_section_order:
            if section_name not in sections:
                continue

            # æŸ¥æ‰¾èµ·å§‹ä½ç½®
            matches = list(re.finditer(start_pattern, full_text, re.IGNORECASE))
            if not matches:
                extracted_sections[section_name] = f"[Section '{section_name}' not found in filing]"
                continue

            # è·³è¿‡ç›®å½•ï¼ˆå‰2%ï¼‰ï¼Œæ‰¾å®é™…å†…å®¹
            best_match = None
            for match in matches:
                pos_pct = match.start() / len(full_text)
                if pos_pct > 0.02:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®è´¨å†…å®¹
                    lookahead = full_text[match.start():match.start() + 500]
                    alpha_count = len([c for c in lookahead if c.isalpha()])
                    if alpha_count > 150:
                        best_match = match
                        break

            if not best_match:
                for match in matches:
                    if match.start() / len(full_text) > 0.02:
                        best_match = match
                        break
                if not best_match and matches:
                    best_match = matches[-1]

            if not best_match:
                extracted_sections[section_name] = f"[Section '{section_name}' not found in filing]"
                continue

            start_pos = best_match.start()

            # ç”¨ç»“æŸæ¨¡å¼æ‰¾è¾¹ç•Œ
            end_match = re.search(end_pattern, full_text[start_pos + 500:], re.IGNORECASE)
            if end_match:
                end_pos = start_pos + 500 + end_match.start()
            else:
                # æ²¡æ‰¾åˆ°ç»“æŸæ¨¡å¼ï¼Œå–å›ºå®šé•¿åº¦
                end_pos = min(start_pos + 80000, len(full_text))

            section_text = full_text[start_pos:end_pos]

            # æ¸…ç†
            section_text = re.sub(r"\s+", " ", section_text).strip()

            # é™åˆ¶é•¿åº¦
            if len(section_text) > 80000:
                section_text = section_text[:80000] + "... [truncated]"

            extracted_sections[section_name] = section_text

        return {
            "file_path": str(file_path),
            "sections": extracted_sections,
            "note": "Extracted from S-1/424B4 prospectus",
        }

    def _infer_quarter(self, report_date: str) -> str:
        """ä»æŠ¥å‘Šæ—¥æœŸæ¨æ–­å­£åº¦ã€‚"""
        month = int(report_date[5:7])
        if month <= 3:
            return "Q1"
        elif month <= 6:
            return "Q2"
        elif month <= 9:
            return "Q3"
        else:
            return "Q4"

    def _update_index(self, filings: List[Dict[str, Any]]) -> None:
        """æ›´æ–° _index.json æ–‡ä»¶ã€‚"""
        # è¯»å–ç°æœ‰ç´¢å¼•
        index = {"ticker": self.ticker, "company_name": "", "updated_at": "", "filings": []}

        if self.index_path.exists():
            with open(self.index_path, "r", encoding="utf-8") as f:
                index = json.load(f)

        # æ›´æ–° filings åˆ—è¡¨ï¼ˆæŒ‰ report_date å»é‡ï¼‰
        existing_dates = {f["report_date"] + "_" + f["filing_type"]: i for i, f in enumerate(index.get("filings", []))}

        for filing in filings:
            key = filing["report_date"] + "_" + filing["filing_type"]
            if key in existing_dates:
                # æ›´æ–°ç°æœ‰è®°å½•
                index["filings"][existing_dates[key]] = filing
            else:
                # æ·»åŠ æ–°è®°å½•
                index["filings"].append(filing)

        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        index["filings"].sort(key=lambda x: x["report_date"], reverse=True)
        index["updated_at"] = datetime.now().isoformat()

        # å†™å…¥æ–‡ä»¶
        with open(self.index_path, "w", encoding="utf-8") as f:
            json.dump(index, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“‹ ç´¢å¼•å·²æ›´æ–°: {self.index_path}")

    def get_index(self) -> Dict[str, Any]:
        """è¯»å–å½“å‰ç´¢å¼•ã€‚"""
        if not self.index_path.exists():
            return {"ticker": self.ticker, "filings": []}

        with open(self.index_path, "r", encoding="utf-8") as f:
            return json.load(f)


def main():
    parser = argparse.ArgumentParser(description="SEC Filing Parser")
    parser.add_argument("--ticker", "-t", required=True, help="è‚¡ç¥¨ä»£ç ï¼Œå¦‚ CRCL")
    parser.add_argument("--filing", "-f", help="æŒ‡å®š filing ç›®å½•åï¼ˆå¯é€‰ï¼Œé»˜è®¤å¤„ç†å…¨éƒ¨ï¼‰")
    parser.add_argument("--list", "-l", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„ filings")

    args = parser.parse_args()

    sec_parser = SECParser(args.ticker)

    if args.list:
        # åˆ—å‡ºå¯ç”¨çš„ filings
        if not sec_parser.filings_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {sec_parser.filings_dir}")
            return

        print(f"\nğŸ“ {args.ticker} å¯ç”¨ Filings:\n")
        for filing_dir in sorted(sec_parser.filings_dir.iterdir()):
            if filing_dir.is_dir():
                print(f"  - {filing_dir.name}")
        print()
        return

    if args.filing:
        # å¤„ç†æŒ‡å®š filing
        result = sec_parser.parse_filing(args.filing)
    else:
        # å¤„ç†å…¨éƒ¨ filings
        result = sec_parser.parse_all()

    print(f"\nğŸ“Š å¤„ç†ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
